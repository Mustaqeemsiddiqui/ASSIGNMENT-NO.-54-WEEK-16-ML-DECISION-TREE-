{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55899fee-b08b-402f-a23d-affad9ce4133",
   "metadata": {},
   "source": [
    "**You are a data scientist working for a healthcare company, and you have been tasked with creating a\n",
    "decision tree to help identify patients with diabetes based on a set of clinical variables. You have been\n",
    "given a dataset (diabetes.csv) with the following variables:**\n",
    "\n",
    "1. Pregnancies: Number of times pregnant (integer)\n",
    "2. Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test (integer)\n",
    "3. BloodPressure: Diastolic blood pressure (mm Hg) (integer)\n",
    "4. SkinThickness: Triceps skin fold thickness (mm) (integer)\n",
    "5. Insulin: 2-Hour serum insulin (mu U/ml) (integer)\n",
    "6. BMI: Body mass index (weight in kg/(height in m)^2) (float)\n",
    "7. DiabetesPedigreeFunction: Diabetes pedigree function (a function which scores likelihood of diabetes\n",
    "based on family history) (float)\n",
    "8. Age: Age in years (integer)\n",
    "9. Outcome: Class variable (0 if non-diabetic, 1 if diabetic) (integer)\n",
    "\n",
    "Hereâ€™s the dataset link:https://drive.google.com/file/d/1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2/view?usp=sharing\n",
    "\n",
    "Your goal is to create a decision tree to predict whether a patient has diabetes based on the other\n",
    "variables. Here are the steps you can follow:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335f44b8-eb1b-4a39-a585-297e496bb2e4",
   "metadata": {},
   "source": [
    "**Q1. Import the dataset and examine the variables. Use descriptive statistics and visualizations to\n",
    "understand the distribution and relationships between the variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1eb126-ada5-4fd9-a691-584cf0bbb42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abef7687-08c0-4f20-8892-b5639c90b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_url = \"https://drive.google.com/uc?id=1Q4J8KS1wm4-_YTuc389enPh6O-eTNcx2\"\n",
    "df = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65f108c-ece2-4733-a03f-7e5aefd83e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
      "0            6      148             72             35        0  33.6   \n",
      "1            1       85             66             29        0  26.6   \n",
      "2            8      183             64              0        0  23.3   \n",
      "3            1       89             66             23       94  28.1   \n",
      "4            0      137             40             35      168  43.1   \n",
      "\n",
      "   DiabetesPedigreeFunction  Age  Outcome  \n",
      "0                     0.627   50        1  \n",
      "1                     0.351   31        0  \n",
      "2                     0.672   32        1  \n",
      "3                     0.167   21        0  \n",
      "4                     2.288   33        1  \n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows and check the structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a4d59f-4cb2-48d5-996e-ba57e87782cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
      "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
      "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
      "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
      "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
      "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
      "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
      "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
      "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
      "\n",
      "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
      "count  768.000000                768.000000  768.000000  768.000000  \n",
      "mean    31.992578                  0.471876   33.240885    0.348958  \n",
      "std      7.884160                  0.331329   11.760232    0.476951  \n",
      "min      0.000000                  0.078000   21.000000    0.000000  \n",
      "25%     27.300000                  0.243750   24.000000    0.000000  \n",
      "50%     32.000000                  0.372500   29.000000    0.000000  \n",
      "75%     36.600000                  0.626250   41.000000    1.000000  \n",
      "max     67.100000                  2.420000   81.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937df93c-6785-4066-9f96-8281c9746a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    " #Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509e2b3-5646-4d72-9c15-c48a9f92d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distributions of numerical variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.pairplot(diabetes_data, hue='Outcome', diag_kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaa564-752c-499c-bbad-00fd7b946f7e",
   "metadata": {},
   "source": [
    "**Q2. Preprocess the data by cleaning missing values, removing outliers, and transforming categorical\n",
    "variables into dummy variables if necessary.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3248763-7838-41c1-9560-043663410094",
   "metadata": {},
   "source": [
    "**ANSWER:--------**\n",
    "\n",
    "\n",
    "To preprocess the data for building a decision tree model, we'll handle missing values, outliers, and transform categorical variables if needed. Here's how we can proceed step by step in Python:\n",
    "\n",
    "### Step 1: Handle Missing Values\n",
    "\n",
    "Let's first check if there are missing values in the dataset and decide on a strategy to handle them. We'll impute missing values for numerical variables (like replacing with mean or median) and handle categorical variables appropriately.\n",
    "\n",
    "\n",
    "\n",
    "If there are missing values, we can handle them using methods like `SimpleImputer` from scikit-learn.\n",
    "\n",
    "### Step 2: Remove Outliers\n",
    "\n",
    "For outliers, we can use statistical methods or visualization to identify and remove them. Here, we'll use a simple approach based on the interquartile range (IQR).\n",
    "\n",
    "\n",
    "### Step 3: Transform Categorical Variables\n",
    "\n",
    "In this dataset, there are no categorical variables that need transformation into dummy variables, as all variables are either numeric or binary (like `Outcome`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ab3f4f3-551a-4c44-8e50-66e6f9592262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "SkinThickness               0\n",
      "Insulin                     0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c02d475-1083-4a64-927e-23a2abb0856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove outliers using IQR\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fc0b707-3f46-4ec3-9b80-c28ef417c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers for numeric columns (if necessary)\n",
    "numeric_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n",
    "for col in numeric_columns:\n",
    "    diabetes_data = remove_outliers(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d627e380-a590-47f8-9834-cf772b1ea03f",
   "metadata": {},
   "source": [
    "**Q3. Split the dataset into a training set and a test set. Use a random seed to ensure reproducibility.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e5ecff4-3518-449c-9b84-6b94714fedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: X_train: (614, 8), y_train: (614,)\n",
      "Test set shape: X_test: (154, 8), y_test: (154,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the features and target variable\n",
    "X = df.drop(columns='Outcome')\n",
    "y = df['Outcome']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"Training set shape: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Test set shape: X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cd4bb9-45f0-4575-acc5-b854e3a0b7ff",
   "metadata": {},
   "source": [
    "**Q4. Use a decision tree algorithm, such as ID3 or C4.5, to train a decision tree model on the training set. Use\n",
    "cross-validation to optimize the hyperparameters and avoid overfitting.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac33718b-f033-4439-b82e-e9d703bc9e0e",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "To train a decision tree model and use cross-validation to optimize the hyperparameters, we'll use scikit-learn's `DecisionTreeClassifier` and `GridSearchCV` for hyperparameter tuning. Let's use the CART (Classification and Regression Trees) algorithm, which is the default in scikit-learn and similar to ID3 and C4.5.\n",
    "\n",
    "### Step-by-Step Approach\n",
    "\n",
    "1. **Import necessary libraries**:\n",
    "   We'll need `DecisionTreeClassifier` for the model and `GridSearchCV` for hyperparameter tuning.\n",
    "\n",
    "2. **Define the parameter grid**:\n",
    "   We'll define a grid of hyperparameters to search over, such as `max_depth`, `min_samples_split`, and `min_samples_leaf`.\n",
    "\n",
    "3. **Perform cross-validation**:\n",
    "   Use `GridSearchCV` to find the best hyperparameters and avoid overfitting.\n",
    "\n",
    "4. **Train the final model**:\n",
    "   Use the best parameters found during cross-validation to train the final decision tree model.\n",
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Parameter Grid**: We define a range of values for `max_depth`, `min_samples_split`, and `min_samples_leaf` to search for the best combination.\n",
    "- **GridSearchCV**: This performs an exhaustive search over the parameter grid using 5-fold cross-validation.\n",
    "- **Best Hyperparameters**: `grid_search.best_params_` will give the best combination of parameters found.\n",
    "- **Best Estimator**: `grid_search.best_estimator_` will give the decision tree model trained with the best parameters.\n",
    "- **Model Evaluation**: We use the test set to evaluate the model's performance with metrics like accuracy, classification report, and confusion matrix.\n",
    "\n",
    "This process ensures that the decision tree model is optimized and less likely to overfit the training data. Let me know if you have any questions or need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db09ca-7b56-4a79-b037-9297f112a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "# Initialize a DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=dtree, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best hyperparameters found by GridSearchCV\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Train the final model with the best hyperparameters\n",
    "best_dtree = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_dtree.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy on test set:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c973f7-cb64-47a9-8c0b-a5e7bd3028d5",
   "metadata": {},
   "source": [
    "**Q5. Evaluate the performance of the decision tree model on the test set using metrics such as accuracy,\n",
    "precision, recall, and F1 score. Use confusion matrices and ROC curves to visualize the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445b6e46-e2df-4187-be62-c2389301f54f",
   "metadata": {},
   "source": [
    "**ANSWER:---------**\n",
    "\n",
    "\n",
    "To evaluate the performance of the decision tree model on the test set, we'll use several metrics: accuracy, precision, recall, F1 score, and the ROC curve. We'll also visualize the results using confusion matrices and the ROC curve.\n",
    "\n",
    "### Step-by-Step Approach\n",
    "\n",
    "1. **Calculate performance metrics**:\n",
    "   We'll use functions from `sklearn.metrics` to calculate accuracy, precision, recall, and F1 score.\n",
    "\n",
    "2. **Generate a confusion matrix**:\n",
    "   This will show the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "3. **Plot the ROC curve**:\n",
    "   The ROC curve and the Area Under the Curve (AUC) will help us understand the model's performance across different thresholds.\n",
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Performance Metrics**: We calculate and print accuracy, precision, recall, and F1 score to evaluate the model's performance.\n",
    "- **Confusion Matrix**: We generate a heatmap of the confusion matrix to visualize true positives, true negatives, false positives, and false negatives.\n",
    "- **ROC Curve**: We plot the ROC curve and calculate the AUC to understand the model's performance across different classification thresholds.\n",
    "\n",
    "This comprehensive evaluation provides a clear understanding of the model's performance on the test set. Let me know if you have any questions or need further assistance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee0b6da-3b0a-43b4-b7ad-ab1cb35010bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Predictions on the test set\n",
    "y_pred = best_dtree.predict(X_test)\n",
    "y_pred_proba = best_dtree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Diabetic', 'Diabetic'], yticklabels=['Non-Diabetic', 'Diabetic'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f68b78-f146-4391-ac06-33c39511a5e3",
   "metadata": {},
   "source": [
    "**Q6. Interpret the decision tree by examining the splits, branches, and leaves. Identify the most important\n",
    "variables and their thresholds. Use domain knowledge and common sense to explain the patterns and\n",
    "trends.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca0c5e-bfae-4429-a9ff-45a79010b789",
   "metadata": {},
   "source": [
    "**ANSWER:-------**\n",
    "\n",
    "\n",
    "To interpret the decision tree, we'll visualize the tree structure and extract feature importances to understand which variables are most influential in predicting diabetes. We can use scikit-learn's `plot_tree` and `feature_importances_` attributes for this purpose.\n",
    "\n",
    "### Step-by-Step Approach\n",
    "\n",
    "1. **Visualize the Decision Tree**:\n",
    "   We'll use `plot_tree` from scikit-learn to visualize the tree structure, including splits, branches, and leaves.\n",
    "\n",
    "2. **Extract Feature Importances**:\n",
    "   We'll extract and display the importance of each feature to understand their influence on the decision-making process.\n",
    "\n",
    "3. **Interpret the Tree**:\n",
    "   Using domain knowledge, we'll interpret the splits and identify the most important variables and their thresholds.\n",
    "\n",
    "\n",
    "\n",
    "### Explanation of Splits and Feature Importances\n",
    "\n",
    "1. **Visualizing the Tree**:\n",
    "   The `plot_tree` function visualizes the entire decision tree. Each node in the tree will display:\n",
    "   - The feature used for the split.\n",
    "   - The threshold value.\n",
    "   - The Gini impurity of the node.\n",
    "   - The number of samples at the node.\n",
    "   - The class distribution at the node.\n",
    "\n",
    "2. **Feature Importances**:\n",
    "   The `feature_importances_` attribute provides the importance of each feature in making predictions. Higher values indicate greater importance.\n",
    "\n",
    "\n",
    "#### Interpretation of the Decision Tree and Feature Importances:\n",
    "\n",
    "- **Glucose**: The most important feature, indicating that plasma glucose concentration is highly predictive of diabetes. High glucose levels are a well-known risk factor for diabetes.\n",
    "- **BMI**: The second most important feature. High BMI is associated with obesity, which is a major risk factor for diabetes.\n",
    "- **Age**: Older age is a significant risk factor for diabetes, as the risk increases with age.\n",
    "- **DiabetesPedigreeFunction**: This measures the genetic risk of diabetes. A higher value indicates a stronger family history, which is an important predictor.\n",
    "- **Insulin**: Abnormal insulin levels are a key indicator of diabetes.\n",
    "- **Pregnancies**: More pregnancies might indicate higher risk, possibly due to gestational diabetes.\n",
    "- **SkinThickness** and **BloodPressure**: These features are less important but still contribute to the overall prediction.\n",
    "\n",
    "By examining the splits, we can identify the thresholds that separate diabetic from non-diabetic patients. For example, a split on Glucose > 130 might indicate a high likelihood of diabetes, which aligns with clinical knowledge.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The decision tree highlights the importance of glucose levels, BMI, and age in predicting diabetes. These variables align with known risk factors, and the tree's splits can be interpreted using clinical knowledge to explain the model's predictions.\n",
    "\n",
    "Let me know if you need further details or have any specific questions about interpreting the decision tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc8e00f-548e-4f95-be00-6c266a38af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(best_dtree, feature_names=X.columns, class_names=['Non-Diabetic', 'Diabetic'], filled=True)\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.show()\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = pd.DataFrame(best_dtree.feature_importances_, index=X.columns, columns=['Importance']).sort_values(by='Importance', ascending=False)\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425012d4-7c52-49b2-9a72-b2b8bea9ad2a",
   "metadata": {},
   "source": [
    "**Q7. Validate the decision tree model by applying it to new data or testing its robustness to changes in the\n",
    "dataset or the environment. Use sensitivity analysis and scenario testing to explore the uncertainty and\n",
    "risks.**\n",
    "\n",
    "By following these steps, you can develop a comprehensive understanding of decision tree modeling and\n",
    "its applications to real-world healthcare problems. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34409413-c44b-46ac-9d0f-968f9d180d68",
   "metadata": {},
   "source": [
    "**ANSWER:------**\n",
    "\n",
    "\n",
    "To validate the decision tree model, we need to assess its performance on new data and test its robustness to changes in the dataset or environment. Sensitivity analysis and scenario testing can help us understand the model's uncertainty and risks.\n",
    "\n",
    "### Step-by-Step Approach\n",
    "\n",
    "1. **Apply the Model to New Data**:\n",
    "   Test the model on a separate validation set or new data to see how well it generalizes.\n",
    "\n",
    "2. **Perform Sensitivity Analysis**:\n",
    "   Analyze how changes in input features affect the model's predictions.\n",
    "\n",
    "3. **Conduct Scenario Testing**:\n",
    "   Create different scenarios to test the model's performance under various conditions.\n",
    "\n",
    "\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **New Data Testing**: This step checks how well the model generalizes to unseen data.\n",
    "- **Sensitivity Analysis**: By varying each feature and observing the changes in predictions, we understand which features the model is most sensitive to.\n",
    "- **Scenario Testing**: Creating extreme or specific scenarios helps assess the model's robustness and identify potential weaknesses.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "These validation steps provide a comprehensive understanding of the decision tree model's robustness and reliability. They help identify potential risks and uncertainties in real-world applications, ensuring that the model performs well under various conditions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478218f-ab76-423d-b729-a71daf93fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_new and y_new are the new data\n",
    "# Predictions on the new data\n",
    "y_new_pred = best_dtree.predict(X_new)\n",
    "\n",
    "# Evaluate the model on new data\n",
    "new_accuracy = accuracy_score(y_new, y_new_pred)\n",
    "new_precision = precision_score(y_new, y_new_pred)\n",
    "new_recall = recall_score(y_new, y_new_pred)\n",
    "new_f1 = f1_score(y_new, y_new_pred)\n",
    "\n",
    "print(f\"Accuracy on new data: {new_accuracy:.2f}\")\n",
    "print(f\"Precision on new data: {new_precision:.2f}\")\n",
    "print(f\"Recall on new data: {new_recall:.2f}\")\n",
    "print(f\"F1 Score on new data: {new_f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ca82d4-00f6-49cc-9bc0-604c0add2331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to perform sensitivity analysis\n",
    "def sensitivity_analysis(model, X, feature_name, feature_range):\n",
    "    base_sample = X.mean().values.reshape(1, -1)  # Base sample with mean values\n",
    "    sensitivities = []\n",
    "\n",
    "    for value in feature_range:\n",
    "        sample = base_sample.copy()\n",
    "        feature_idx = list(X.columns).index(feature_name)\n",
    "        sample[0, feature_idx] = value\n",
    "        prediction = model.predict(sample)[0]\n",
    "        sensitivities.append((value, prediction))\n",
    "\n",
    "    return sensitivities\n",
    "\n",
    "# Sensitivity analysis for 'Glucose'\n",
    "glucose_range = np.linspace(X['Glucose'].min(), X['Glucose'].max(), 100)\n",
    "glucose_sensitivities = sensitivity_analysis(best_dtree, X_train, 'Glucose', glucose_range)\n",
    "\n",
    "# Plotting the sensitivity analysis\n",
    "glucose_values, predictions = zip(*glucose_sensitivities)\n",
    "plt.plot(glucose_values, predictions, marker='o')\n",
    "plt.xlabel('Glucose')\n",
    "plt.ylabel('Prediction (0 = Non-Diabetic, 1 = Diabetic)')\n",
    "plt.title('Sensitivity Analysis for Glucose')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa94ac-5af7-4f57-9c35-41bce7893519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario testing by creating extreme conditions\n",
    "extreme_conditions = pd.DataFrame({\n",
    "    'Pregnancies': [10, 0],\n",
    "    'Glucose': [200, 50],\n",
    "    'BloodPressure': [100, 60],\n",
    "    'SkinThickness': [50, 10],\n",
    "    'Insulin': [300, 20],\n",
    "    'BMI': [50.0, 18.5],\n",
    "    'DiabetesPedigreeFunction': [2.5, 0.1],\n",
    "    'Age': [70, 20]\n",
    "})\n",
    "\n",
    "# Predictions under extreme conditions\n",
    "extreme_predictions = best_dtree.predict(extreme_conditions)\n",
    "\n",
    "print(\"Extreme Conditions Predictions:\")\n",
    "print(extreme_conditions)\n",
    "print(\"Predictions:\", extreme_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfa690-88d0-4bd0-aed0-c366b59c4c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
